"""ä¸‹è½½ Embedding æ¨¡å‹åˆ°æœ¬åœ°ï¼Œç”¨äºç¦»çº¿ä½¿ç”¨ã€‚

è¿è¡Œæ­¤è„šæœ¬åï¼Œæ¨¡å‹å°†è¢«ä¸‹è½½åˆ°æœ¬åœ°ï¼Œä¹‹åå¯ä»¥å®Œå…¨ç¦»çº¿è¿è¡Œã€‚

ä½¿ç”¨æ–¹æ³•:
    python -m src.core.rag.download_embedding_model
    æˆ–è€…
    python winclaw/src/core/rag/download_embedding_model.py
"""

import os
import shutil
from pathlib import Path


# é»˜è®¤æ¨¡å‹
DEFAULT_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# æœ¬åœ°æ¨¡å‹å­˜å‚¨ç›®å½•ï¼ˆæ”¾åœ¨é¡¹ç›®çš„ resources ç›®å½•ä¸‹ï¼‰
LOCAL_MODEL_DIR = Path(__file__).parent.parent.parent.parent / "resources" / "embedding_models"


def download_model(model_name: str = DEFAULT_MODEL, force: bool = False) -> Path:
    """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç›®å½•ã€‚

    Args:
        model_name: HuggingFace æ¨¡å‹åç§°
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½

    Returns:
        æœ¬åœ°æ¨¡å‹è·¯å¾„
    """
    from huggingface_hub import snapshot_download
    from huggingface_hub.utils import RepositoryNotFoundError

    # å°†æ¨¡å‹åè½¬æ¢ä¸ºç›®å½•åï¼ˆæ›¿æ¢ / ä¸º _ï¼‰
    model_dir_name = model_name.replace("/", "_")
    local_path = LOCAL_MODEL_DIR / model_dir_name

    if local_path.exists() and not force:
        print(f"âœ… æ¨¡å‹å·²å­˜åœ¨: {local_path}")
        return local_path

    print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
    print(f"ğŸ“ ç›®æ ‡è·¯å¾„: {local_path}")

    try:
        # ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
        downloaded_path = snapshot_download(
            repo_id=model_name,
            local_dir=str(local_path),
            local_dir_use_symlinks=False,  # Windows ä¸Šé¿å…ç¬¦å·é“¾æ¥é—®é¢˜
            resume_download=True,
        )
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {downloaded_path}")
        return Path(downloaded_path)

    except RepositoryNotFoundError:
        print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_name}")
        raise
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        raise


def verify_model(model_path: Path) -> bool:
    """éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ã€‚

    Args:
        model_path: æ¨¡å‹è·¯å¾„

    Returns:
        æ˜¯å¦å®Œæ•´
    """
    required_files = [
        "config.json",
        "pytorch_model.bin",  # æˆ– model.safetensors
        "tokenizer.json",
        "tokenizer_config.json",
        "vocab.txt",
        "special_tokens_map.json",
    ]

    # æ£€æŸ¥è‡³å°‘å­˜åœ¨ä¸€ç§æ¨¡å‹æ–‡ä»¶
    has_model = (model_path / "pytorch_model.bin").exists() or \
                (model_path / "model.safetensors").exists()

    if not has_model:
        print(f"âŒ ç¼ºå°‘æ¨¡å‹æƒé‡æ–‡ä»¶")
        return False

    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not (model_path / "config.json").exists():
        print(f"âŒ ç¼ºå°‘ config.json")
        return False

    # æ£€æŸ¥ tokenizer æ–‡ä»¶
    if not (model_path / "tokenizer.json").exists():
        print(f"âš ï¸ è­¦å‘Š: ç¼ºå°‘ tokenizer.json")

    print(f"âœ… æ¨¡å‹æ–‡ä»¶éªŒè¯é€šè¿‡")
    return True


def get_model_info(model_path: Path) -> dict:
    """è·å–æ¨¡å‹ä¿¡æ¯ã€‚

    Args:
        model_path: æ¨¡å‹è·¯å¾„

    Returns:
        æ¨¡å‹ä¿¡æ¯å­—å…¸
    """
    import json

    info = {
        "path": str(model_path),
        "exists": model_path.exists(),
    }

    config_path = model_path / "config.json"
    if config_path.exists():
        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)
            info["hidden_size"] = config.get("hidden_size")
            info["max_position_embeddings"] = config.get("max_position_embeddings")
            info["model_type"] = config.get("model_type")

    # è®¡ç®—ç›®å½•å¤§å°
    if model_path.exists():
        total_size = sum(f.stat().st_size for f in model_path.rglob("*") if f.is_file())
        info["size_mb"] = round(total_size / 1024 / 1024, 2)

    return info


def main():
    """ä¸»å‡½æ•°ã€‚"""
    import argparse

    parser = argparse.ArgumentParser(description="ä¸‹è½½ Embedding æ¨¡å‹åˆ°æœ¬åœ°")
    parser.add_argument(
        "--model",
        default=DEFAULT_MODEL,
        help=f"æ¨¡å‹åç§°ï¼Œé»˜è®¤: {DEFAULT_MODEL}"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°ä¸‹è½½"
    )
    parser.add_argument(
        "--info",
        action="store_true",
        help="åªæ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯ï¼Œä¸ä¸‹è½½"
    )

    args = parser.parse_args()

    model_dir_name = args.model.replace("/", "_")
    local_path = LOCAL_MODEL_DIR / model_dir_name

    if args.info:
        info = get_model_info(local_path)
        print("\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        for key, value in info.items():
            print(f"  {key}: {value}")
        return

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    LOCAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)

    # ä¸‹è½½æ¨¡å‹
    model_path = download_model(args.model, force=args.force)

    # éªŒè¯æ¨¡å‹
    if verify_model(model_path):
        info = get_model_info(model_path)
        print(f"\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        print(f"  è·¯å¾„: {info['path']}")
        print(f"  å¤§å°: {info.get('size_mb', 'N/A')} MB")
        print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print(f"  åœ¨ embedder.py ä¸­è®¾ç½®:")
        print(f"  LOCAL_MODEL_PATH = r\"{model_path}\"")
        print(f"  æˆ–è®¾ç½®ç¯å¢ƒå˜é‡:")
        print(f"  set EMBEDDING_MODEL_PATH={model_path}")


if __name__ == "__main__":
    main()
