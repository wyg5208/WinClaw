"""è¯­éŸ³è¾“å…¥å·¥å…· - åŸºäº Whisper çš„è¯­éŸ³è½¬æ–‡å­—

æ”¯æŒ:
- å®æ—¶å½•éŸ³ï¼ˆç›´æ¥ä¼  numpy æ•°ç»„ç»™ Whisperï¼Œæ— éœ€ ffmpegï¼‰
- éŸ³é¢‘æ–‡ä»¶è½¬æ–‡å­—ï¼ˆWAV å¯ç”¨ scipy è¯»å–ï¼Œå…¶ä»–æ ¼å¼éœ€ ffmpegï¼‰
- çº¯å½•éŸ³å¹¶ä¿å­˜æ–‡ä»¶ï¼ˆrecord_audioï¼‰
- VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼Œè¯´å®Œè‡ªåŠ¨åœæ­¢
- å¤šè¯­è¨€è¯†åˆ«
- å¯é€‰æ¨¡å‹å¤§å° (tiny/base/small/medium/large)

Phase 4.6 ä¼˜åŒ–ï¼š
- å»¶è¿Ÿå¯¼å…¥ï¼šwhisper/sounddevice/numpy/scipy ä»…åœ¨å®é™…ä½¿ç”¨æ—¶å¯¼å…¥
- å¯åŠ¨é€Ÿåº¦å¤§å¹…æå‡

Phase 7.0 ä¼˜åŒ–ï¼š
- æ–°å¢ record_audio åŠ¨ä½œï¼šçº¯å½•éŸ³ç”Ÿæˆ WAV æ–‡ä»¶
- VADï¼ˆVoice Activity Detectionï¼‰æ™ºèƒ½åœæ­¢ï¼šè¯´å®Œè‡ªåŠ¨åœæ­¢å½•éŸ³
- ç§»é™¤å›ºå®š5ç§’é™åˆ¶ï¼Œæ”¯æŒçµæ´»æ—¶é•¿
"""
import asyncio
import logging
import os
import shutil
import tempfile
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

logger = logging.getLogger(__name__)

# å»¶è¿Ÿå¯¼å…¥æ ‡è®°
VOICE_AVAILABLE: bool | None = None
FFMPEG_AVAILABLE: bool | None = None

# æ¨¡å—å¼•ç”¨ï¼ˆå»¶è¿ŸåŠ è½½åèµ‹å€¼ï¼‰
_whisper = None
_sd = None
_np = None
_read_wav = None
_write_wav = None


def _check_voice_dependencies() -> bool:
    """æ£€æŸ¥è¯­éŸ³ä¾èµ–æ˜¯å¦å¯ç”¨ï¼Œå»¶è¿Ÿå¯¼å…¥ã€‚"""
    global VOICE_AVAILABLE, _whisper, _sd, _np, _read_wav, _write_wav
    if VOICE_AVAILABLE is not None:
        return VOICE_AVAILABLE

    try:
        import whisper
        import sounddevice as sd
        import numpy as np
        from scipy.io.wavfile import read as read_wav
        from scipy.io.wavfile import write as write_wav

        _whisper = whisper
        _sd = sd
        _np = np
        _read_wav = read_wav
        _write_wav = write_wav
        VOICE_AVAILABLE = True
        logger.debug("è¯­éŸ³ä¾èµ–åŠ è½½æˆåŠŸ")
    except ImportError:
        VOICE_AVAILABLE = False
        logger.debug("è¯­éŸ³ä¾èµ–ä¸å¯ç”¨")

    return VOICE_AVAILABLE


def _check_ffmpeg() -> bool:
    """æ£€æµ‹ ffmpeg æ˜¯å¦å¯ç”¨ã€‚"""
    global FFMPEG_AVAILABLE
    if FFMPEG_AVAILABLE is None:
        FFMPEG_AVAILABLE = shutil.which("ffmpeg") is not None
    return FFMPEG_AVAILABLE


from .base import ActionDef, BaseTool, ToolResult, ToolResultStatus
from .text_utils import to_simplified_chinese


class VoiceInputTool(BaseTool):
    """è¯­éŸ³è¾“å…¥å·¥å…· - ä½¿ç”¨ Whisper å°†è¯­éŸ³è½¬ä¸ºæ–‡å­—"""

    name = "voice_input"
    emoji = "ğŸ¤"
    title = "è¯­éŸ³è¾“å…¥"
    description = "è¯­éŸ³è½¬æ–‡å­—å·¥å…·,æ”¯æŒå®æ—¶å½•éŸ³ã€VADæ™ºèƒ½å½•éŸ³ã€çº¯å½•éŸ³ä¿å­˜æ–‡ä»¶æˆ–ä»éŸ³é¢‘æ–‡ä»¶è¯†åˆ«"

    # VADï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰é»˜è®¤å‚æ•°
    VAD_SILENCE_THRESHOLD = 0.01    # é™éŸ³èƒ½é‡é˜ˆå€¼
    VAD_SILENCE_DURATION = 1.5      # é™éŸ³æŒç»­æ—¶é—´(ç§’)è§¦å‘åœæ­¢
    VAD_MIN_RECORDING = 1.0         # æœ€çŸ­å½•éŸ³æ—¶é•¿(ç§’)
    VAD_MAX_RECORDING = 30.0        # æœ€å¤§å½•éŸ³æ—¶é•¿(ç§’)
    VAD_CHUNK_DURATION = 0.1        # æ¯æ¬¡æ£€æµ‹å—æ—¶é•¿(ç§’)

    def __init__(self):
        super().__init__()
        self._model: Optional[Any] = None
        self._model_name: str = "base"
        self._sample_rate: int = 16000
        # å½•éŸ³ä¸­æ­¢æ ‡å¿—ï¼ˆä¾›å¤–éƒ¨åœæ­¢å½•éŸ³ï¼‰
        self._stop_recording = False
        # ä¸åœ¨åˆå§‹åŒ–æ—¶æ£€æŸ¥ä¾èµ–ï¼Œå»¶è¿Ÿåˆ°å®é™…ä½¿ç”¨æ—¶

    def _check_available(self) -> bool:
        """æ£€æŸ¥è¯­éŸ³åŠŸèƒ½æ˜¯å¦å¯ç”¨ã€‚"""
        if not _check_voice_dependencies():
            raise ImportError(
                "è¯­éŸ³åŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·å®‰è£…ä¾èµ–: pip install openai-whisper sounddevice scipy"
            )
        return True

    def _load_model(self, model_name: str = "base") -> Any:
        """å»¶è¿ŸåŠ è½½ Whisper æ¨¡å‹"""
        self._check_available()
        if self._model is None or self._model_name != model_name:
            self._model_name = model_name
            self._model = _whisper.load_model(model_name)
        return self._model

    def get_actions(self) -> list[ActionDef]:
        return [
            ActionDef(
                name="record_and_transcribe",
                description="å½•åˆ¶éŸ³é¢‘å¹¶è½¬ä¸ºæ–‡å­—ï¼ˆæ”¯æŒVADè‡ªåŠ¨åœæ­¢ï¼šè¯´å®Œè‡ªåŠ¨åœæ­¢å½•éŸ³ï¼‰",
                parameters={
                    "duration": {
                        "type": "number",
                        "description": "æœ€å¤§å½•éŸ³æ—¶é•¿(ç§’),VADæ¨¡å¼ä¸‹ä¸ºä¸Šé™,é»˜è®¤30ç§’",
                        "default": 30,
                    },
                    "auto_stop": {
                        "type": "boolean",
                        "description": "æ˜¯å¦å¯ç”¨VADè‡ªåŠ¨æ£€æµ‹è¯´è¯ç»“æŸ,é»˜è®¤True",
                        "default": True,
                    },
                    "model": {
                        "type": "string",
                        "description": "Whisper æ¨¡å‹ (tiny/base/small/medium/large),é»˜è®¤ base",
                        "default": "base",
                        "enum": ["tiny", "base", "small", "medium", "large"],
                    },
                    "language": {
                        "type": "string",
                        "description": "è¯­è¨€ä»£ç (å¦‚ zh/en),ç•™ç©ºè‡ªåŠ¨æ£€æµ‹",
                        "default": None,
                    },
                },
                required_params=[],
            ),
            ActionDef(
                name="record_audio",
                description="å½•åˆ¶éŸ³é¢‘å¹¶ä¿å­˜ä¸ºWAVæ–‡ä»¶ï¼ˆæ”¯æŒVADè‡ªåŠ¨åœæ­¢ï¼‰",
                parameters={
                    "duration": {
                        "type": "number",
                        "description": "æœ€å¤§å½•éŸ³æ—¶é•¿(ç§’),é»˜è®¤30ç§’",
                        "default": 30,
                    },
                    "auto_stop": {
                        "type": "boolean",
                        "description": "æ˜¯å¦å¯ç”¨VADè‡ªåŠ¨æ£€æµ‹è¯´è¯ç»“æŸ,é»˜è®¤True",
                        "default": True,
                    },
                    "save_path": {
                        "type": "string",
                        "description": "ä¿å­˜è·¯å¾„(ç•™ç©ºè‡ªåŠ¨ç”Ÿæˆåˆ° generated/audio/ ç›®å½•)",
                        "default": None,
                    },
                },
                required_params=[],
            ),
            ActionDef(
                name="transcribe_file",
                description="å°†éŸ³é¢‘æ–‡ä»¶è½¬ä¸ºæ–‡å­—",
                parameters={
                    "file_path": {
                        "type": "string",
                        "description": "éŸ³é¢‘æ–‡ä»¶è·¯å¾„(æ”¯æŒ wav/mp3/m4a ç­‰)",
                    },
                    "model": {
                        "type": "string",
                        "description": "Whisper æ¨¡å‹",
                        "default": "base",
                        "enum": ["tiny", "base", "small", "medium", "large"],
                    },
                    "language": {
                        "type": "string",
                        "description": "è¯­è¨€ä»£ç ,ç•™ç©ºè‡ªåŠ¨æ£€æµ‹",
                        "default": None,
                    },
                },
                required_params=["file_path"],
            ),
            ActionDef(
                name="list_devices",
                description="åˆ—å‡ºå¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡",
                parameters={},
                required_params=[],
            ),
        ]

    async def execute(self, action: str, params: dict[str, Any]) -> ToolResult:
        """æ‰§è¡Œè¯­éŸ³è¾“å…¥æ“ä½œ"""
        if action == "record_and_transcribe":
            return await self._record_and_transcribe(**params)
        elif action == "record_audio":
            return await self._record_audio(**params)
        elif action == "transcribe_file":
            return await self._transcribe_file(**params)
        elif action == "list_devices":
            return self._list_devices()
        else:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"æœªçŸ¥åŠ¨ä½œ: {action}",
                output=f"å¯ç”¨åŠ¨ä½œ: {[a.name for a in self.get_actions()]}",
            )

    def stop_recording(self) -> None:
        """å¤–éƒ¨è¯·æ±‚åœæ­¢å½“å‰å½•éŸ³ï¼ˆæ‰‹åŠ¨åœæ­¢æŒ‰é’®è°ƒç”¨ï¼‰ã€‚"""
        self._stop_recording = True

    def _record_with_vad(
        self,
        max_duration: float = 30.0,
        auto_stop: bool = True,
        silence_threshold: float = VAD_SILENCE_THRESHOLD,
        silence_duration: float = VAD_SILENCE_DURATION,
    ) -> tuple:
        """ä½¿ç”¨ VADï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰å½•éŸ³ã€‚

        åœ¨åå°çº¿ç¨‹ä¸­åŒæ­¥æ‰§è¡Œã€‚æŒç»­å½•éŸ³ç›´åˆ°æ£€æµ‹åˆ°è¯´å®Œï¼ˆé™éŸ³è¶…è¿‡é˜ˆå€¼ï¼‰ï¼Œ
        æˆ–è¾¾åˆ°æœ€å¤§æ—¶é•¿ï¼Œæˆ–å¤–éƒ¨è°ƒç”¨ stop_recording()ã€‚

        Args:
            max_duration: æœ€å¤§å½•éŸ³æ—¶é•¿(ç§’)
            auto_stop: æ˜¯å¦å¯ç”¨VADè‡ªåŠ¨åœæ­¢
            silence_threshold: é™éŸ³èƒ½é‡é˜ˆå€¼
            silence_duration: é™éŸ³æŒç»­å¤šå°‘ç§’åœæ­¢

        Returns:
            (audio_data: numpy float32 array, actual_duration: float)
        """
        self._stop_recording = False
        chunk_samples = int(self.VAD_CHUNK_DURATION * self._sample_rate)
        max_samples = int(max_duration * self._sample_rate)
        min_samples = int(self.VAD_MIN_RECORDING * self._sample_rate)
        silence_samples_needed = int(silence_duration / self.VAD_CHUNK_DURATION)

        all_chunks = []
        total_samples = 0
        silence_count = 0
        has_speech = False

        logger.info(
            "å¼€å§‹VADå½•éŸ³: max=%.1fs, auto_stop=%s, threshold=%.4f, silence=%.1fs",
            max_duration, auto_stop, silence_threshold, silence_duration,
        )

        # æ‰“å¼€éŸ³é¢‘æµ
        stream = _sd.InputStream(
            samplerate=self._sample_rate,
            channels=1,
            dtype="float32",
            blocksize=chunk_samples,
        )
        stream.start()

        try:
            while total_samples < max_samples and not self._stop_recording:
                chunk, overflowed = stream.read(chunk_samples)
                all_chunks.append(chunk.copy())
                total_samples += len(chunk)

                # è®¡ç®— RMS èƒ½é‡
                energy = float(_np.sqrt(_np.mean(chunk ** 2)))

                if energy > silence_threshold:
                    silence_count = 0
                    has_speech = True
                else:
                    silence_count += 1

                # VAD è‡ªåŠ¨åœæ­¢ï¼šå·²ç»æœ‰è¯­éŸ³è¾“å…¥ï¼Œä¸”è¿ç»­é™éŸ³è¶…è¿‡é˜ˆå€¼
                if auto_stop and has_speech and total_samples >= min_samples:
                    if silence_count >= silence_samples_needed:
                        logger.info("VAD æ£€æµ‹åˆ°è¯´è¯ç»“æŸï¼ˆé™éŸ³ %.1f ç§’ï¼‰", silence_count * self.VAD_CHUNK_DURATION)
                        break
        finally:
            stream.stop()
            stream.close()

        if not all_chunks:
            return _np.array([], dtype=_np.float32), 0.0

        audio_data = _np.concatenate(all_chunks, axis=0).flatten().astype(_np.float32)
        actual_duration = len(audio_data) / self._sample_rate
        logger.info("å½•éŸ³å®Œæˆ: å®é™…æ—¶é•¿=%.1fs, æ•°æ®é•¿åº¦=%d", actual_duration, len(audio_data))
        return audio_data, actual_duration

    async def _record_and_transcribe(
        self, duration: float = 30.0, auto_stop: bool = True,
        model: str = "base", language: Optional[str] = None
    ) -> ToolResult:
        """å½•åˆ¶éŸ³é¢‘å¹¶è½¬æ–‡å­—ï¼ˆæ”¯æŒ VAD è‡ªåŠ¨åœæ­¢ï¼‰ã€‚

        Args:
            duration: æœ€å¤§å½•éŸ³æ—¶é•¿(ç§’), VADæ¨¡å¼ä¸‹ä¸ºä¸Šé™
            auto_stop: æ˜¯å¦å¯ç”¨ VAD è‡ªåŠ¨æ£€æµ‹è¯´è¯ç»“æŸ
            model: Whisper æ¨¡å‹
            language: è¯­è¨€ä»£ç 
        """
        try:
            # æ£€æŸ¥ä¾èµ–
            self._check_available()

            # é™åˆ¶æ—¶é•¿
            duration = max(1, min(duration, 120))

            logger.info("å¼€å§‹å½•éŸ³: max=%.1fs, auto_stop=%s, é‡‡æ ·ç‡=%d",
                        duration, auto_stop, self._sample_rate)

            # åœ¨çº¿ç¨‹æ± ä¸­ä½¿ç”¨ VAD å½•éŸ³
            loop = asyncio.get_event_loop()
            audio_data, actual_duration = await loop.run_in_executor(
                None,
                lambda: self._record_with_vad(
                    max_duration=duration,
                    auto_stop=auto_stop,
                )
            )

            if len(audio_data) == 0 or actual_duration < 0.3:
                return ToolResult(
                    status=ToolResultStatus.SUCCESS,
                    output="æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³",
                    data={"text": "", "language": "unknown", "duration": actual_duration},
                )

            logger.info("å½•éŸ³å®Œæˆ, å®é™…æ—¶é•¿: %.1fs, æ•°æ®é•¿åº¦: %d, èŒƒå›´: [%.4f, %.4f]",
                        actual_duration, len(audio_data), audio_data.min(), audio_data.max())

            # åŠ è½½æ¨¡å‹
            model_obj = await loop.run_in_executor(None, self._load_model, model)

            # ç›´æ¥å°† numpy æ•°ç»„ä¼ ç»™ Whisperï¼ˆæ— éœ€ ffmpegï¼‰
            transcribe_kwargs = {"fp16": False}
            if language:
                transcribe_kwargs["language"] = language

            result = await loop.run_in_executor(
                None, lambda: model_obj.transcribe(audio_data, **transcribe_kwargs)
            )

            text = result["text"].strip()
            detected_language = result.get("language", "unknown")

            # è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡
            text = to_simplified_chinese(text)

            logger.info("è½¬å½•å®Œæˆ: è¯­è¨€=%s, æ–‡å­—=%s", detected_language, text[:50])

            return ToolResult(
                status=ToolResultStatus.SUCCESS,
                output=f"å½•éŸ³è½¬å½•æˆåŠŸ (æ—¶é•¿: {actual_duration:.1f}s, è¯­è¨€: {detected_language})",
                data={
                    "text": text,
                    "language": detected_language,
                    "duration": actual_duration,
                    "model": model,
                    "auto_stopped": auto_stop,
                },
            )

        except Exception as e:
            logger.exception("å½•éŸ³è½¬å½•å¤±è´¥")
            return ToolResult(status=ToolResultStatus.ERROR, error=f"å½•éŸ³è½¬å½•å¤±è´¥: {e}")

    async def _record_audio(
        self, duration: float = 30.0, auto_stop: bool = True,
        save_path: Optional[str] = None,
    ) -> ToolResult:
        """çº¯å½•éŸ³å¹¶ä¿å­˜ä¸º WAV æ–‡ä»¶ã€‚

        Args:
            duration: æœ€å¤§å½•éŸ³æ—¶é•¿(ç§’)
            auto_stop: æ˜¯å¦å¯ç”¨ VAD è‡ªåŠ¨åœæ­¢
            save_path: ä¿å­˜è·¯å¾„(None åˆ™è‡ªåŠ¨ç”Ÿæˆ)
        """
        try:
            self._check_available()

            duration = max(1, min(duration, 120))

            # ç¡®å®šä¿å­˜è·¯å¾„
            if save_path:
                out_path = Path(save_path).expanduser().resolve()
            else:
                # è‡ªåŠ¨ç”Ÿæˆåˆ° generated/audio/ ç›®å½•
                audio_dir = Path.cwd() / "generated" / "audio"
                audio_dir.mkdir(parents=True, exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                out_path = audio_dir / f"recording_{timestamp}.wav"

            logger.info("å¼€å§‹çº¯å½•éŸ³: max=%.1fs, auto_stop=%s, ä¿å­˜åˆ°=%s",
                        duration, auto_stop, out_path)

            # VAD å½•éŸ³
            loop = asyncio.get_event_loop()
            audio_data, actual_duration = await loop.run_in_executor(
                None,
                lambda: self._record_with_vad(
                    max_duration=duration,
                    auto_stop=auto_stop,
                )
            )

            if len(audio_data) == 0 or actual_duration < 0.3:
                return ToolResult(
                    status=ToolResultStatus.SUCCESS,
                    output="æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³ï¼Œæœªä¿å­˜æ–‡ä»¶",
                    data={"file_path": None, "duration": 0},
                )

            # ä¿å­˜ä¸º WAV æ–‡ä»¶ï¼ˆint16 æ ¼å¼ï¼‰
            out_path.parent.mkdir(parents=True, exist_ok=True)
            audio_int16 = (_np.clip(audio_data, -1.0, 1.0) * 32767).astype(_np.int16)
            await loop.run_in_executor(
                None,
                lambda: _write_wav(str(out_path), self._sample_rate, audio_int16)
            )

            file_size_mb = out_path.stat().st_size / (1024 * 1024)
            logger.info("å½•éŸ³æ–‡ä»¶å·²ä¿å­˜: %s (%.2f MB, %.1fs)", out_path, file_size_mb, actual_duration)

            return ToolResult(
                status=ToolResultStatus.SUCCESS,
                output=f"å½•éŸ³å·²ä¿å­˜: {out_path.name} ({actual_duration:.1f}s, {file_size_mb:.2f}MB)",
                data={
                    "file_path": str(out_path),
                    "duration": actual_duration,
                    "file_size_mb": file_size_mb,
                    "sample_rate": self._sample_rate,
                    "format": "wav",
                },
            )

        except Exception as e:
            logger.exception("å½•éŸ³ä¿å­˜å¤±è´¥")
            return ToolResult(status=ToolResultStatus.ERROR, error=f"å½•éŸ³ä¿å­˜å¤±è´¥: {e}")

    def _load_audio_file(self, file_path: str):
        """åŠ è½½éŸ³é¢‘æ–‡ä»¶ä¸º Whisper è¦æ±‚çš„ float32 numpy æ•°ç»„ã€‚

        ä¼˜å…ˆä½¿ç”¨ ffmpegï¼ˆæ”¯æŒæ‰€æœ‰æ ¼å¼ï¼‰ï¼Œè‹¥ä¸å¯ç”¨åˆ™ç”¨ scipy è¯»å– WAVã€‚
        """
        self._check_available()

        if _check_ffmpeg():
            # ffmpeg å¯ç”¨æ—¶ï¼Œä½¿ç”¨ whisper å†…ç½®åŠ è½½ï¼ˆæ”¯æŒæ‰€æœ‰æ ¼å¼ï¼‰
            return _whisper.load_audio(file_path)

        # ffmpeg ä¸å¯ç”¨ï¼Œç”¨ scipy è¯»å– WAV æ–‡ä»¶
        ext = Path(file_path).suffix.lower()
        if ext not in (".wav", ".wave"):
            raise RuntimeError(
                f"ä¸æ”¯æŒ {ext} æ ¼å¼ï¼ˆéœ€è¦ ffmpegï¼‰ã€‚"
                f"è¯·å®‰è£… ffmpeg æˆ–å°†æ–‡ä»¶è½¬ä¸º WAV æ ¼å¼ã€‚\n"
                f"å®‰è£…æ–¹æ³•: winget install Gyan.FFmpeg"
            )

        sample_rate, data = _read_wav(file_path)

        # è½¬ä¸º float32
        if data.dtype == _np.int16:
            audio = data.astype(_np.float32) / 32768.0
        elif data.dtype == _np.int32:
            audio = data.astype(_np.float32) / 2147483648.0
        elif data.dtype == _np.float32:
            audio = data
        else:
            audio = data.astype(_np.float32)

        # å¤šå£°é“è½¬å•å£°é“
        if audio.ndim > 1:
            audio = audio.mean(axis=1)

        # é‡é‡‡æ ·åˆ° 16kHz (Whisper è¦æ±‚)
        if sample_rate != 16000:
            # ç®€å•çº¿æ€§é‡é‡‡æ ·
            duration = len(audio) / sample_rate
            target_len = int(duration * 16000)
            indices = _np.linspace(0, len(audio) - 1, target_len)
            audio = _np.interp(indices, _np.arange(len(audio)), audio).astype(_np.float32)

        return audio

    async def _transcribe_file(
        self, file_path: str, model: str = "base", language: Optional[str] = None
    ) -> ToolResult:
        """å°†éŸ³é¢‘æ–‡ä»¶è½¬ä¸ºæ–‡å­—"""
        try:
            path = Path(file_path).expanduser().resolve()
            if not path.exists():
                return ToolResult(status=ToolResultStatus.ERROR, error=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            # æ£€æŸ¥æ–‡ä»¶å¤§å° (é™åˆ¶ 50MB)
            file_size_mb = path.stat().st_size / (1024 * 1024)
            if file_size_mb > 50:
                return ToolResult(
                    status=ToolResultStatus.ERROR, error=f"æ–‡ä»¶è¿‡å¤§: {file_size_mb:.1f}MB (é™åˆ¶ 50MB)"
                )

            # åŠ è½½æ¨¡å‹
            loop = asyncio.get_event_loop()
            model_obj = await loop.run_in_executor(None, self._load_model, model)

            # åŠ è½½éŸ³é¢‘æ–‡ä»¶ä¸º numpy æ•°ç»„
            audio_data = await loop.run_in_executor(None, self._load_audio_file, str(path))

            # è½¬å½•ï¼ˆä¼ å…¥ numpy æ•°ç»„ï¼Œæ— éœ€ ffmpegï¼‰
            transcribe_kwargs = {"fp16": False}
            if language:
                transcribe_kwargs["language"] = language

            result = await loop.run_in_executor(
                None, lambda: model_obj.transcribe(audio_data, **transcribe_kwargs)
            )

            text = result["text"].strip()
            detected_language = result.get("language", "unknown")

            # è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡
            text = to_simplified_chinese(text)

            ffmpeg_note = "" if _check_ffmpeg() else " (æ—  ffmpeg, ä»…æ”¯æŒ WAV)"
            return ToolResult(
                status=ToolResultStatus.SUCCESS,
                output=f"æ–‡ä»¶è½¬å½•æˆåŠŸ: {path.name}{ffmpeg_note}",
                data={
                    "text": text,
                    "language": detected_language,
                    "file_path": str(path),
                    "file_size_mb": file_size_mb,
                    "model": model,
                },
            )

        except Exception as e:
            logger.exception("æ–‡ä»¶è½¬å½•å¤±è´¥")
            return ToolResult(status=ToolResultStatus.ERROR, error=f"æ–‡ä»¶è½¬å½•å¤±è´¥: {e}")

    def _list_devices(self) -> ToolResult:
        """åˆ—å‡ºå¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡"""
        try:
            self._check_available()

            devices = _sd.query_devices()
            input_devices = []

            for i, dev in enumerate(devices):
                if dev["max_input_channels"] > 0:
                    input_devices.append(
                        {
                            "index": i,
                            "name": dev["name"],
                            "channels": dev["max_input_channels"],
                            "sample_rate": dev["default_samplerate"],
                        }
                    )

            default_device = _sd.query_devices(kind="input")

            return ToolResult(
                status=ToolResultStatus.SUCCESS,
                output=f"æ‰¾åˆ° {len(input_devices)} ä¸ªéŸ³é¢‘è¾“å…¥è®¾å¤‡",
                data={"devices": input_devices, "default": default_device["name"]},
            )

        except Exception as e:
            return ToolResult(status=ToolResultStatus.ERROR, error=f"æŸ¥è¯¢è®¾å¤‡å¤±è´¥: {e}")
