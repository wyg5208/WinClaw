"""Search å·¥å…· â€” æœ¬åœ°æ–‡ä»¶æœç´¢ + Web æœç´¢ï¼ˆSprint 2.3ï¼‰ã€‚

æ”¯æŒåŠ¨ä½œï¼š
- local_search: æœ¬åœ°æ–‡ä»¶å/å†…å®¹æœç´¢ï¼ˆglob éå†ï¼‰
- web_search: Web æœç´¢ï¼ˆä½¿ç”¨ DuckDuckGo HTML è§£æï¼Œæ— éœ€ API Keyï¼‰
"""

from __future__ import annotations

import asyncio
import fnmatch
import logging
import re
import urllib.parse
import urllib.request
from pathlib import Path
from typing import Any

from src.tools.base import ActionDef, BaseTool, ToolResult, ToolResultStatus

logger = logging.getLogger(__name__)


class SearchTool(BaseTool):
    """æœ¬åœ°æ–‡ä»¶æœç´¢ + Web æœç´¢å·¥å…·ã€‚"""

    name = "search"
    emoji = "ğŸ”"
    title = "æœç´¢"
    description = "æœç´¢æœ¬åœ°æ–‡ä»¶ï¼ˆæŒ‰æ–‡ä»¶åæˆ–å†…å®¹ï¼‰å’Œ Web ç½‘é¡µæœç´¢"

    def __init__(
        self,
        max_local_results: int = 50,
        max_web_results: int = 10,
        local_max_depth: int = 5,
        web_timeout: int = 10,
        search_engine: str = "auto",  # auto, bing, baidu, duckduckgo
        max_results_per_page: int = 50,  # å•æ¬¡æœ€å¤§ç»“æœæ•°ï¼ˆBingé™åˆ¶ï¼‰
    ):
        self.max_local_results = max_local_results
        self.max_web_results = max_web_results
        self.local_max_depth = local_max_depth
        self.web_timeout = web_timeout
        self.search_engine = search_engine
        self.max_results_per_page = max_results_per_page

    def get_actions(self) -> list[ActionDef]:
        return [
            ActionDef(
                name="local_search",
                description="åœ¨æœ¬åœ°ç›®å½•ä¸­æœç´¢æ–‡ä»¶ã€‚æ”¯æŒæŒ‰æ–‡ä»¶åæ¨¡å¼ï¼ˆglobï¼‰æˆ–æ–‡ä»¶å†…å®¹å…³é”®è¯æœç´¢ã€‚",
                parameters={
                    "directory": {
                        "type": "string",
                        "description": "æœç´¢èµ·å§‹ç›®å½•è·¯å¾„",
                    },
                    "pattern": {
                        "type": "string",
                        "description": "æ–‡ä»¶ååŒ¹é…æ¨¡å¼ï¼ˆglob æ ¼å¼ï¼Œå¦‚ '*.pdf', '*.txt', 'report*'ï¼‰",
                    },
                    "content": {
                        "type": "string",
                        "description": "æœç´¢æ–‡ä»¶å†…å®¹ä¸­åŒ…å«çš„å…³é”®è¯ï¼ˆå¯é€‰ï¼Œä¸ pattern å¯åŒæ—¶ä½¿ç”¨ï¼‰",
                    },
                    "max_depth": {
                        "type": "integer",
                        "description": "æœ€å¤§æœç´¢ç›®å½•æ·±åº¦ï¼ˆé»˜è®¤5ï¼‰",
                    },
                },
                required_params=["directory"],
            ),
            ActionDef(
                name="web_search",
                description="åœ¨äº’è”ç½‘ä¸Šæœç´¢ä¿¡æ¯ã€‚ä½¿ç”¨æœç´¢å¼•æ“æŸ¥æ‰¾ç›¸å…³ç½‘é¡µï¼Œè¿”å›æ ‡é¢˜ã€æ‘˜è¦å’Œé“¾æ¥ã€‚",
                parameters={
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯",
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "æœ€å¤§è¿”å›ç»“æœæ•°ï¼ˆé»˜è®¤10ï¼Œæœ€å¤š50ï¼‰",
                        "default": 10,
                    },
                    "page": {
                        "type": "integer",
                        "description": "é¡µç ï¼ˆä»1å¼€å§‹ï¼Œé»˜è®¤1ï¼‰ï¼Œç”¨äºç¿»é¡µæŸ¥çœ‹æ›´å¤šç»“æœ",
                        "default": 1,
                    },
                },
                required_params=["query"],
            ),
        ]

    async def execute(self, action: str, params: dict[str, Any]) -> ToolResult:
        handlers = {
            "local_search": self._local_search,
            "web_search": self._web_search,
        }
        handler = handlers.get(action)
        if handler is None:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"ä¸æ”¯æŒçš„åŠ¨ä½œ: {action}",
            )
        return await handler(params)

    # ------------------------------------------------------------------
    # local_search
    # ------------------------------------------------------------------

    async def _local_search(self, params: dict[str, Any]) -> ToolResult:
        directory = params.get("directory", "").strip()
        pattern = params.get("pattern", "*")
        content_keyword = params.get("content", "").strip()
        max_depth = params.get("max_depth", self.local_max_depth)

        if not directory:
            return ToolResult(status=ToolResultStatus.ERROR, error="æœç´¢ç›®å½•ä¸èƒ½ä¸ºç©º")

        search_dir = Path(directory).expanduser().resolve()
        if not search_dir.exists():
            return ToolResult(status=ToolResultStatus.ERROR, error=f"ç›®å½•ä¸å­˜åœ¨: {search_dir}")
        if not search_dir.is_dir():
            return ToolResult(status=ToolResultStatus.ERROR, error=f"è·¯å¾„ä¸æ˜¯ç›®å½•: {search_dir}")

        results: list[dict[str, Any]] = []

        def _walk(dir_path: Path, depth: int) -> None:
            if depth > max_depth or len(results) >= self.max_local_results:
                return
            try:
                for entry in sorted(dir_path.iterdir(), key=lambda p: p.name.lower()):
                    if len(results) >= self.max_local_results:
                        return
                    if entry.is_dir():
                        # è·³è¿‡éšè—ç›®å½•å’Œå¸¸è§æ— å…³ç›®å½•
                        if entry.name.startswith(".") or entry.name in (
                            "__pycache__", "node_modules", ".git", "venv", ".venv"
                        ):
                            continue
                        _walk(entry, depth + 1)
                    elif entry.is_file():
                        if not fnmatch.fnmatch(entry.name.lower(), pattern.lower()):
                            continue

                        file_info: dict[str, Any] = {
                            "path": str(entry),
                            "name": entry.name,
                            "size": entry.stat().st_size,
                        }

                        # å†…å®¹æœç´¢
                        if content_keyword:
                            try:
                                text = entry.read_text(encoding="utf-8", errors="ignore")
                                if content_keyword.lower() not in text.lower():
                                    continue
                                # æ‰¾åˆ°åŒ¹é…çš„è¡Œ
                                matched_lines = []
                                for i, line in enumerate(text.splitlines(), 1):
                                    if content_keyword.lower() in line.lower():
                                        matched_lines.append(f"  L{i}: {line.strip()[:100]}")
                                        if len(matched_lines) >= 3:
                                            break
                                file_info["matched_lines"] = matched_lines
                            except (UnicodeDecodeError, PermissionError, OSError):
                                continue

                        results.append(file_info)
            except PermissionError:
                pass

        # åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œæ–‡ä»¶éå†ä»¥é¿å…é˜»å¡
        await asyncio.get_event_loop().run_in_executor(None, _walk, search_dir, 1)

        if not results:
            msg = f"åœ¨ {search_dir} ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶"
            if pattern != "*":
                msg += f"ï¼ˆæ¨¡å¼: {pattern}ï¼‰"
            if content_keyword:
                msg += f"ï¼ˆå†…å®¹å«: {content_keyword}ï¼‰"
            return ToolResult(
                status=ToolResultStatus.SUCCESS,
                output=msg,
                data={"results": [], "count": 0},
            )

        lines = [f"åœ¨ {search_dir} ä¸­æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…æ–‡ä»¶:\n"]
        for r in results:
            size = r["size"]
            if size < 1024:
                size_str = f"{size}B"
            elif size < 1_048_576:
                size_str = f"{size / 1024:.1f}KB"
            else:
                size_str = f"{size / 1_048_576:.1f}MB"

            lines.append(f"  ğŸ“„ {r['name']} ({size_str})")
            lines.append(f"     {r['path']}")
            if "matched_lines" in r:
                for ml in r["matched_lines"]:
                    lines.append(f"    {ml}")

        if len(results) >= self.max_local_results:
            lines.append(f"\n  ...(è¾¾åˆ°ä¸Šé™ {self.max_local_results}ï¼Œå¯èƒ½è¿˜æœ‰æ›´å¤š)")

        logger.info("æœ¬åœ°æœç´¢: %s pattern=%s content=%s â†’ %d ç»“æœ",
                     search_dir, pattern, content_keyword or "(æ— )", len(results))
        return ToolResult(
            status=ToolResultStatus.SUCCESS,
            output="\n".join(lines),
            data={"results": [{"path": r["path"], "name": r["name"]} for r in results],
                  "count": len(results)},
        )

    # ------------------------------------------------------------------
    # web_search
    # ------------------------------------------------------------------

    async def _web_search(self, params: dict[str, Any]) -> ToolResult:
        query = params.get("query", "").strip()
        max_results = params.get("max_results", self.max_web_results)
        page = params.get("page", 1)

        if not query:
            return ToolResult(status=ToolResultStatus.ERROR, error="æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º")

        # é™åˆ¶å‚æ•°èŒƒå›´
        max_results = max(1, min(max_results, self.max_results_per_page))
        page = max(1, min(page, 10))  # æœ€å¤š10é¡µ

        # å°è¯•å¤šä¸ªæœç´¢å¼•æ“
        engines = []
        if self.search_engine == "auto":
            # ä¼˜å…ˆBingï¼ˆæœ€ç¨³å®šï¼‰ï¼Œå…¶æ¬¡ç™¾åº¦ï¼Œæœ€åDuckDuckGo
            engines = ["bing", "baidu", "duckduckgo"]
        else:
            engines = [self.search_engine]

        results = []
        last_error = None
        
        for engine in engines:
            try:
                logger.info("å°è¯•ä½¿ç”¨ %s æœç´¢ (page=%d, max_results=%d)...", engine, page, max_results)
                fetch_func = getattr(self, f"_fetch_{engine}", None)
                if fetch_func is None:
                    continue
                    
                results = await asyncio.get_event_loop().run_in_executor(
                    None, fetch_func, query, max_results, page
                )
                
                if results:
                    logger.info("%s æœç´¢æˆåŠŸ: '%s' â†’ %d ç»“æœ", engine, query, len(results))
                    break
            except Exception as e:
                last_error = e
                logger.warning("%s æœç´¢å¤±è´¥: %s", engine, e)
                continue

        if not results:
            error_msg = f"æ‰€æœ‰æœç´¢å¼•æ“å‡å¤±è´¥"
            if last_error:
                error_msg += f": {last_error}"
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=error_msg,
                output="å»ºè®®: 1. æ£€æŸ¥ç½‘ç»œè¿æ¥ 2. å°è¯•ä½¿ç”¨ä»£ç† 3. ç¨åé‡è¯•",
            )

        lines = [f"æœç´¢ '{query}' çš„ç»“æœ (å…± {len(results)} æ¡):\n"]
        if page > 1:
            lines[0] = f"æœç´¢ '{query}' çš„ç»“æœ - ç¬¬ {page} é¡µ (å…± {len(results)} æ¡):\n"
        
        for i, r in enumerate(results, 1):
            lines.append(f"  {i}. {r['title']}")
            lines.append(f"     {r['url']}")
            if r.get("snippet"):
                lines.append(f"     {r['snippet'][:150]}")
            lines.append("")

        return ToolResult(
            status=ToolResultStatus.SUCCESS,
            output="\n".join(lines),
            data={
                "results": results, 
                "count": len(results), 
                "engine": engine,
                "page": page,
                "max_results": max_results,
                "has_more": len(results) >= max_results,  # å¯èƒ½è¿˜æœ‰æ›´å¤šç»“æœ
            },
        )

    def _fetch_bing(self, query: str, max_results: int, page: int = 1) -> list[dict[str, str]]:
        """é€šè¿‡ Bing æœç´¢ï¼ˆæ— éœ€ API Keyï¼‰ã€‚
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æ¯é¡µç»“æœæ•°
            page: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
        """
        encoded_query = urllib.parse.quote_plus(query)
        
        # è®¡ç®—åç§»é‡: Bing ä½¿ç”¨ first å‚æ•° (0, 10, 20, ...)
        first = (page - 1) * max_results
        url = f"https://www.bing.com/search?q={encoded_query}&count={max_results}&first={first}"

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        }
        req = urllib.request.Request(url, headers=headers)

        try:
            with urllib.request.urlopen(req, timeout=self.web_timeout) as resp:
                html = resp.read().decode("utf-8", errors="replace")
        except Exception as e:
            logger.warning("Bing è¯·æ±‚å¤±è´¥: %s", e)
            raise

        results: list[dict[str, str]] = []

        # Bing ç»“æœè§£æ - æ›´çµæ´»çš„åŒ¹é…
        # å…ˆæŸ¥æ‰¾æ‰€æœ‰ b_algo ç»“æœå—
        algo_blocks = re.findall(
            r'<li[^>]*class="[^"]*b_algo[^"]*"[^>]*>(.*?)</li>',
            html,
            re.DOTALL,
        )

        for block in algo_blocks[:max_results]:
            # æå–æ ‡é¢˜å’ŒURL
            title_match = re.search(r'<h2[^>]*>.*?<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>', block, re.DOTALL)
            if not title_match:
                continue
            
            href = title_match.group(1)
            title_html = title_match.group(2)
            title = re.sub(r"<[^>]+>", "", title_html).strip()
            
            # æå–æ‘˜è¦ - å°è¯•å¤šç§æ–¹å¼
            snippet = ""
            
            # æ–¹å¼1: æŸ¥æ‰¾ <p> æ ‡ç­¾
            p_match = re.search(r'<p[^>]*>(.*?)</p>', block, re.DOTALL)
            if p_match:
                snippet = re.sub(r"<[^>]+>", "", p_match.group(1)).strip()
            
            # æ–¹å¼2: æŸ¥æ‰¾ class åŒ…å« caption æˆ– description
            if not snippet:
                desc_match = re.search(r'class="[^"]*(?:caption|description)[^"]*"[^>]*>(.*?)</div>', block, re.DOTALL)
                if desc_match:
                    snippet = re.sub(r"<[^>]+>", "", desc_match.group(1)).strip()

            if title and href and href.startswith("http"):
                results.append({
                    "title": title,
                    "url": href,
                    "snippet": snippet,
                })

        return results

    def _fetch_baidu(self, query: str, max_results: int, page: int = 1) -> list[dict[str, str]]:
        """é€šè¿‡ç™¾åº¦æœç´¢ï¼ˆæ— éœ€ API Keyï¼‰ã€‚
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æ¯é¡µç»“æœæ•°
            page: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
        """
        encoded_query = urllib.parse.quote(query)
        
        # è®¡ç®—åç§»é‡: ç™¾åº¦ä½¿ç”¨ pn å‚æ•° (0, 10, 20, ...)
        pn = (page - 1) * max_results
        url = f"https://www.baidu.com/s?wd={encoded_query}&rn={max_results}&pn={pn}"

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9",
        }
        req = urllib.request.Request(url, headers=headers)

        try:
            with urllib.request.urlopen(req, timeout=self.web_timeout) as resp:
                html = resp.read().decode("utf-8", errors="replace")
        except Exception as e:
            logger.warning("ç™¾åº¦è¯·æ±‚å¤±è´¥: %s", e)
            raise

        results: list[dict[str, str]] = []

        # ç™¾åº¦ç»“æœè§£æ - æ›´å®½æ¾çš„åŒ¹é…
        # åŒ¹é…å„ç§ç‰ˆæœ¬çš„ç™¾åº¦ç»“æœé¡µé¢
        result_blocks = re.findall(
            r'<div[^>]*class="[^"]*result[^"]*"[^>]*>.*?<h3[^>]*>.*?<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>.*?</h3>(.*?)</div>',
            html,
            re.DOTALL,
        )

        for href, title_html, content_html in result_blocks[:max_results * 3]:  # å¤šæŠ½å–ä¸€äº›,è¿‡æ»¤åå¯èƒ½ä¸å¤Ÿ
            title = re.sub(r"<[^>]+>", "", title_html).strip()
            
            # æå–æ‘˜è¦ï¼ˆå¯èƒ½åœ¨ä¸åŒçš„æ ‡ç­¾ä¸­ï¼‰
            snippet = ""
            snippet_match = re.search(r'class="[^"]*abstract[^"]*"[^>]*>(.*?)</div>', content_html, re.DOTALL)
            if snippet_match:
                snippet = re.sub(r"<[^>]+>", "", snippet_match.group(1)).strip()
            else:
                # å¤‡é€‰ï¼šç›´æ¥ä» content ä¸­æå–
                snippet = re.sub(r"<[^>]+>", "", content_html).strip()[:200]

            # è¿‡æ»¤å¹¿å‘Šå’Œæ— æ•ˆé“¾æ¥
            if not title or len(title) < 3:
                continue
            if "å¹¿å‘Š" in title or "æ¨å¹¿" in title:
                continue
            
            # å¤„ç†ç™¾åº¦é‡å®šå‘é“¾æ¥
            real_url = href
            if "baidu.com" in href and ("link?" in href or "baidu.php" in href):
                # å°è¯•æå–çœŸå® URL
                url_match = re.search(r"url=([^&]+)", href)
                if url_match:
                    try:
                        real_url = urllib.parse.unquote(url_match.group(1))
                    except Exception:
                        pass
                # å¦‚æœä»ç„¶æ˜¯ç™¾åº¦é“¾æ¥,ä¿ç•™åŸå§‹é“¾æ¥
                if not real_url.startswith("http") or "baidu.com" in real_url:
                    real_url = href

            if title:
                results.append({
                    "title": title,
                    "url": real_url,
                    "snippet": snippet,
                })
                
            if len(results) >= max_results:
                break

        return results

    def _fetch_duckduckgo(self, query: str, max_results: int, page: int = 1) -> list[dict[str, str]]:
        """é€šè¿‡ DuckDuckGo HTML é¡µé¢è§£ææœç´¢ç»“æœï¼ˆæ— éœ€ API Keyï¼‰ã€‚
        
        æ³¨: DuckDuckGo HTML ç‰ˆæœ¬ä¸æ”¯æŒåˆ†é¡µ,åªèƒ½è¿”å›ç¬¬ä¸€é¡µã€‚
        """
        if page > 1:
            # DuckDuckGo HTML ç‰ˆä¸æ”¯æŒåˆ†é¡µ
            return []
            
        encoded_query = urllib.parse.quote_plus(query)
        url = f"https://html.duckduckgo.com/html/?q={encoded_query}"

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        }
        req = urllib.request.Request(url, headers=headers)

        try:
            with urllib.request.urlopen(req, timeout=self.web_timeout) as resp:
                html = resp.read().decode("utf-8", errors="replace")
        except Exception as e:
            logger.warning("DuckDuckGo è¯·æ±‚å¤±è´¥: %s", e)
            raise

        results: list[dict[str, str]] = []

        # è§£ææœç´¢ç»“æœ â€” DuckDuckGo HTML ç‰ˆæœ¬çš„ç»“æ„
        # æ¯ä¸ªç»“æœåœ¨ <a class="result__a" href="...">title</a> å’Œ <a class="result__snippet">snippet</a>
        result_blocks = re.findall(
            r'<a\s+rel="nofollow"\s+class="result__a"[^>]*href="([^"]*)"[^>]*>(.*?)</a>'
            r'.*?<a\s+class="result__snippet"[^>]*>(.*?)</a>',
            html,
            re.DOTALL,
        )

        for href, title_html, snippet_html in result_blocks[:max_results]:
            # æ¸…ç† HTML æ ‡ç­¾
            title = re.sub(r"<[^>]+>", "", title_html).strip()
            snippet = re.sub(r"<[^>]+>", "", snippet_html).strip()

            # DuckDuckGo çš„ href æ˜¯é‡å®šå‘é“¾æ¥ï¼Œå°è¯•æå–çœŸå® URL
            real_url = href
            uddg_match = re.search(r"uddg=([^&]+)", href)
            if uddg_match:
                real_url = urllib.parse.unquote(uddg_match.group(1))

            if title and real_url:
                results.append({
                    "title": title,
                    "url": real_url,
                    "snippet": snippet,
                })

        return results
