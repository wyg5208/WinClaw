"""RAG çŸ¥è¯†åº“å·¥å…· - åŸºäºå‘é‡æ£€ç´¢çš„æ™ºèƒ½çŸ¥è¯†åº“ã€‚

æä¾›åŠ¨ä½œï¼š
- add_document: æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼ˆè§£æ + å‘é‡åŒ– + å­˜å‚¨ï¼‰
- search: è¯­ä¹‰æœç´¢çŸ¥è¯†åº“
- query_document: æŸ¥è¯¢æŒ‡å®šæ–‡æ¡£å†…å®¹
- list_documents: åˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£
- remove_document: åˆ é™¤æ–‡æ¡£

ä¾èµ–ï¼š
- chromadb: å‘é‡æ•°æ®åº“
- sentence-transformers: æœ¬åœ° Embedding
- pymupdf4llm: PDF è§£æ
- python-docx: Word è§£æ
- beautifulsoup4: URL è§£æ

è®¾è®¡ï¼š
- ä½¿ç”¨æœ¬åœ°å‘é‡å­˜å‚¨ï¼Œä¿æŠ¤æ•°æ®éšç§
- æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼
- ä¸ç°æœ‰ knowledge.py å·¥å…·å…±å­˜
"""

import logging
import os
import shutil
import uuid
from dataclasses import asdict
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

from src.tools.base import ActionDef, BaseTool, ToolResult, ToolResultStatus

logger = logging.getLogger(__name__)

# é»˜è®¤é…ç½®
_DEFAULT_DB_DIR = os.path.expanduser("~/.winclaw")
_DOC_DIR = os.path.join(_DEFAULT_DB_DIR, "documents")
_VECTOR_DB_DIR = os.path.join(_DEFAULT_DB_DIR, "chroma_db")

# æ”¯æŒçš„æ–‡ä»¶ç±»å‹
_SUPPORTED_TYPES = {
    "pdf", "docx", "doc", "pptx", "ppt",
    "txt", "md", "markdown",
    "json", "csv", "xlsx", "xls",
    "jpg", "jpeg", "png", "gif", "webp", "bmp",
}

# æœ€å¤§æ–‡ä»¶å¤§å° (50MB)
_MAX_FILE_SIZE = 50 * 1024 * 1024


class KnowledgeRAGTool(BaseTool):
    """RAG çŸ¥è¯†åº“å·¥å…·ã€‚"""

    name = "knowledge_rag"
    emoji = "ğŸ§ "
    title = "æ™ºèƒ½çŸ¥è¯†åº“"
    description = "åŸºäºå‘é‡æ£€ç´¢çš„æ™ºèƒ½çŸ¥è¯†åº“ï¼Œæ”¯æŒ PDF/DOCX/å›¾ç‰‡/URL ç­‰æ ¼å¼"

    def __init__(
        self,
        db_path: str = "",
        doc_dir: str = "",
        vector_db_dir: str = "",
        vision_client=None,
    ):
        """åˆå§‹åŒ– RAG çŸ¥è¯†åº“å·¥å…·ã€‚

        Args:
            db_path: SQLite æ•°æ®åº“è·¯å¾„ï¼ˆå­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®ï¼‰
            doc_dir: æ–‡æ¡£å­˜å‚¨ç›®å½•
            vector_db_dir: ChromaDB å‘é‡æ•°æ®åº“ç›®å½•
            vision_client: è§†è§‰æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆç”¨äºå›¾ç‰‡å¤„ç†ï¼‰
        """
        super().__init__()

        self._db_path = db_path or os.path.join(_DEFAULT_DB_DIR, "winclaw_rag.db")
        self._doc_dir = Path(doc_dir or _DOC_DIR)
        self._vector_db_dir = vector_db_dir or _VECTOR_DB_DIR

        self._doc_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ– RAG ç»„ä»¶
        self._embedder = None
        self._vector_store = None
        self._parser = None
        self._vision_client = vision_client

        # åˆå§‹åŒ– SQLite
        self._init_db()

    @property
    def embedder(self):
        """è·å–åµŒå…¥å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰ã€‚"""
        if self._embedder is None:
            from src.core.rag import Embedder
            self._embedder = Embedder()
        return self._embedder

    @property
    def vector_store(self):
        """è·å–å‘é‡å­˜å‚¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰ã€‚"""
        if self._vector_store is None:
            from src.core.rag import VectorStore
            self._vector_store = VectorStore(
                db_path=self._vector_db_dir,
                embedding_function=self.embedder,
            )
        return self._vector_store

    @property
    def parser(self):
        """è·å–æ–‡æ¡£è§£æå™¨ã€‚"""
        if self._parser is None:
            from src.core.rag import DocumentParser
            self._parser = DocumentParser(
                vision_client=self._vision_client,
            )
        return self._parser

    def _init_db(self) -> None:
        """åˆå§‹åŒ– SQLite æ•°æ®åº“ã€‚"""
        import sqlite3

        conn = sqlite3.connect(self._db_path)
        try:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS rag_documents (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    filename TEXT NOT NULL,
                    original_path TEXT NOT NULL,
                    stored_path TEXT NOT NULL,
                    file_type TEXT NOT NULL,
                    file_size INTEGER DEFAULT 0,
                    content_text TEXT,
                    chunk_count INTEGER DEFAULT 0,
                    indexed_at TEXT NOT NULL,
                    updated_at TEXT NOT NULL
                )
            """)
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_rag_filename
                ON rag_documents(filename)
            """)
            conn.commit()
        finally:
            conn.close()

    def get_actions(self) -> list[ActionDef]:
        return [
            ActionDef(
                name="add_document",
                description=(
                    "å°†æ–‡æ¡£æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚æ”¯æŒ PDF/DOCX/PPT/TXT/MD/JSON/CSV/å›¾ç‰‡ç­‰æ ¼å¼ã€‚"
                    "ä¼šè‡ªåŠ¨è§£ææ–‡æ¡£å†…å®¹ã€åˆ†å—ã€å‘é‡åŒ–å¹¶å­˜å…¥å‘é‡æ•°æ®åº“ã€‚"
                ),
                parameters={
                    "file_path": {
                        "type": "string",
                        "description": "è¦æ·»åŠ çš„æ–‡æ¡£è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰",
                    },
                    "url": {
                        "type": "string",
                        "description": "æˆ–è€…è¾“å…¥ç½‘é¡µ URLï¼ˆå¯é€‰ï¼Œä¸ file_path äºŒé€‰ä¸€ï¼‰",
                    },
                },
                required_params=["file_path"],
            ),
            ActionDef(
                name="search",
                description=(
                    "è¯­ä¹‰æœç´¢çŸ¥è¯†åº“ã€‚æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢ç›¸å…³æ–‡æ¡£å†…å®¹ã€‚"
                    "è¿”å›ä¸é—®é¢˜æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚"
                ),
                parameters={
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å†…å®¹/é—®é¢˜",
                    },
                    "top_k": {
                        "type": "integer",
                        "description": "è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤ 3",
                    },
                },
                required_params=["query"],
            ),
            ActionDef(
                name="query_document",
                description=(
                    "æŸ¥è¯¢æŒ‡å®šæ–‡æ¡£çš„å†…å®¹ã€‚"
                    "æ ¹æ®å…³é”®è¯æ£€ç´¢ç‰¹å®šæ–‡æ¡£ä¸­çš„ç›¸å…³å†…å®¹ã€‚"
                ),
                parameters={
                    "document_name": {
                        "type": "string",
                        "description": "æ–‡æ¡£åï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼‰",
                    },
                    "query": {
                        "type": "string",
                        "description": "æŸ¥è¯¢å†…å®¹/é—®é¢˜",
                    },
                    "top_k": {
                        "type": "integer",
                        "description": "è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤ 3",
                    },
                },
                required_params=["document_name", "query"],
            ),
            ActionDef(
                name="list_documents",
                description="åˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£",
                parameters={
                    "limit": {
                        "type": "integer",
                        "description": "è¿”å›æ•°é‡ï¼Œé»˜è®¤ 50",
                    },
                },
                required_params=[],
            ),
            ActionDef(
                name="remove_document",
                description="ä»çŸ¥è¯†åº“ä¸­åˆ é™¤æŒ‡å®šæ–‡æ¡£",
                parameters={
                    "document_id": {
                        "type": "integer",
                        "description": "æ–‡æ¡£ ID",
                    },
                },
                required_params=["document_id"],
            ),
        ]

    async def execute(self, action: str, params: dict[str, Any]) -> ToolResult:
        handlers = {
            "add_document": self._add_document,
            "search": self._search,
            "query_document": self._query_document,
            "list_documents": self._list_documents,
            "remove_document": self._remove_document,
        }

        handler = handlers.get(action)
        if handler is None:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"ä¸æ”¯æŒçš„åŠ¨ä½œ: {action}",
            )

        try:
            return handler(params)
        except Exception as e:
            import traceback
            logger.error(f"çŸ¥è¯†åº“æ“ä½œå¤±è´¥: {e}\n{traceback.format_exc()}")
            return ToolResult(status=ToolResultStatus.ERROR, error=str(e))

    # -------------------- åŠ¨ä½œå®ç° --------------------

    def _add_document(self, params: dict[str, Any]) -> ToolResult:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ã€‚"""
        file_path = params.get("file_path", "").strip()
        url = params.get("url", "").strip()

        now = datetime.now().isoformat()

        # å¤„ç† URL
        if url:
            return self._add_url(url, now)

        # å¤„ç†æ–‡ä»¶
        if not file_path:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error="å¿…é¡»æä¾› file_path æˆ– url",
            )

        fp = Path(file_path)

        # éªŒè¯æ–‡ä»¶
        if not fp.exists():
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}",
            )

        if not fp.is_file():
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"ä¸æ˜¯æ–‡ä»¶: {file_path}",
            )

        ext = fp.suffix.lower().lstrip(".")
        if ext not in _SUPPORTED_TYPES:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}ï¼Œæ”¯æŒçš„ç±»å‹: {', '.join(_SUPPORTED_TYPES)}",
            )

        file_size = fp.stat().st_size
        if file_size > _MAX_FILE_SIZE:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"æ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.1f}MBï¼Œæœ€å¤§æ”¯æŒ {_MAX_FILE_SIZE / 1024 / 1024}MB",
            )

        # è§£ææ–‡æ¡£
        parse_result = self.parser.parse(file_path)

        if not parse_result.success:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"æ–‡æ¡£è§£æå¤±è´¥: {parse_result.error}",
            )

        # æ£€æŸ¥è§£æå†…å®¹æ˜¯å¦ä¸ºç©º
        if not parse_result.content or len(parse_result.content.strip()) == 0:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"æ–‡æ¡£è§£ææˆåŠŸä½†å†…å®¹ä¸ºç©ºï¼Œå¯èƒ½æ˜¯åŠ å¯†PDFæˆ–å›¾ç‰‡PDFï¼Œè¯·å°è¯•å…¶ä»–æ–¹å¼æå–æ–‡å­—",
            )

        # å¤åˆ¶æ–‡ä»¶åˆ°å­˜å‚¨ç›®å½•
        stored_filename = f"{uuid.uuid4()}_{fp.name}"
        stored_path = self._doc_dir / stored_filename
        shutil.copy2(fp, stored_path)

        # åˆ†å—
        from src.core.rag import TextSplitter
        splitter = TextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = splitter.split(
            parse_result.content,
            metadata={"filename": fp.name, "file_type": parse_result.file_type},
        )

        chunk_metadatas = []
        
        # å‘é‡åŒ–å¹¶å­˜å‚¨
        if chunks:
            chunk_texts = [chunk.text for chunk in chunks]
            chunk_metadatas = [
                {
                    "doc_id": 0,  # ä¸´æ—¶ï¼Œç¨åæ›´æ–°
                    "filename": fp.name,
                    "file_type": parse_result.file_type,
                    "chunk_index": chunk.chunk_index,
                }
                for chunk in chunks
            ]

            # æ·»åŠ åˆ°å‘é‡åº“
            chunk_ids = self.vector_store.add_documents(
                documents=chunk_texts,
                metadatas=chunk_metadatas,
            )

        # ä¿å­˜åˆ°æ•°æ®åº“
        import sqlite3

        conn = sqlite3.connect(self._db_path)
        try:
            cursor = conn.execute(
                """INSERT INTO rag_documents
                   (filename, original_path, stored_path, file_type, file_size,
                    content_text, chunk_count, indexed_at, updated_at)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                (
                    fp.name,
                    str(fp.resolve()),
                    str(stored_path),
                    parse_result.file_type,
                    file_size,
                    parse_result.content[:10000],  # é™åˆ¶å­˜å‚¨å†…å®¹å¤§å°
                    len(chunks),
                    now,
                    now,
                ),
            )
            doc_id = cursor.lastrowid

            # æ›´æ–°å‘é‡åº“ä¸­çš„ doc_id
            if chunks:
                for i in range(len(chunks)):
                    chunk_metadatas[i]["doc_id"] = doc_id

                # é‡æ–°æ·»åŠ ï¼ˆå®é™…ä¸Šåº”è¯¥ç›´æ¥ä½¿ç”¨æ­£ç¡® doc_idï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                # TODO: ä¼˜åŒ–ä¸ºç›´æ¥ä½¿ç”¨æ­£ç¡® doc_id

            conn.commit()
        finally:
            conn.close()

        return ToolResult(
            status=ToolResultStatus.SUCCESS,
            output=(
                f"âœ… æ–‡æ¡£å·²æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼š{fp.name}\n"
                f"   ç±»å‹: {parse_result.file_type}\n"
                f"   å¤§å°: {file_size / 1024:.1f}KB\n"
                f"   å—æ•°: {len(chunks)}"
            ),
            data={
                "document_id": doc_id,
                "filename": fp.name,
                "file_type": parse_result.file_type,
                "chunk_count": len(chunks),
            },
        )

    def _add_url(self, url: str, now: str) -> ToolResult:
        """æ·»åŠ  URL åˆ°çŸ¥è¯†åº“ã€‚"""
        # éªŒè¯ URL
        from urllib.parse import urlparse

        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"æ— æ•ˆçš„ URL: {url}",
            )

        # è§£æ URL
        try:
            parse_result = self.parser.parse_url(url)
        except Exception as e:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"URL è§£æå¤±è´¥: {e}",
            )

        if not parse_result.content:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error="æ— æ³•æå–ç½‘é¡µå†…å®¹",
            )

        # ä¿å­˜ URL å†…å®¹åˆ°æ–‡ä»¶
        url_filename = f"{uuid.uuid4()}_{parsed.netloc}.txt"
        stored_path = self._doc_dir / url_filename
        content = f"# {parse_result.title}\n\nURL: {url}\n\n{parse_result.content}"
        stored_path.write_text(content, encoding="utf-8")

        # åˆ†å—
        from src.core.rag import TextSplitter
        splitter = TextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = splitter.split(
            content,
            metadata={"filename": url, "file_type": "url"},
        )

        chunk_metadatas = []
        
        # å­˜å‚¨åˆ°å‘é‡åº“
        if chunks:
            chunk_texts = [chunk.text for chunk in chunks]
            chunk_metadatas = [
                {
                    "doc_id": 0,
                    "filename": url,
                    "file_type": "url",
                    "chunk_index": chunk.chunk_index,
                    "source_url": url,
                }
                for chunk in chunks
            ]

            self.vector_store.add_documents(
                documents=chunk_texts,
                metadatas=chunk_metadatas,
            )

        # ä¿å­˜åˆ°æ•°æ®åº“
        import sqlite3

        conn = sqlite3.connect(self._db_path)
        try:
            cursor = conn.execute(
                """INSERT INTO rag_documents
                   (filename, original_path, stored_path, file_type, file_size,
                    content_text, chunk_count, indexed_at, updated_at)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                (
                    url,
                    url,
                    str(stored_path),
                    "url",
                    len(content),
                    content[:10000],
                    len(chunks),
                    now,
                    now,
                ),
            )
            doc_id = cursor.lastrowid
            conn.commit()
        finally:
            conn.close()

        return ToolResult(
            status=ToolResultStatus.SUCCESS,
            output=(
                f"âœ… ç½‘é¡µå·²æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼š{parse_result.title or url}\n"
                f"   ç½‘å€: {url}\n"
                f"   å—æ•°: {len(chunks)}"
            ),
            data={
                "document_id": doc_id,
                "filename": url,
                "title": parse_result.title,
                "chunk_count": len(chunks),
            },
        )

    def _search(self, params: dict[str, Any]) -> ToolResult:
        """è¯­ä¹‰æœç´¢ã€‚"""
        query = params.get("query", "").strip()
        top_k = params.get("top_k", 3)

        if not query:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error="æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º",
            )

        try:
            results = self.vector_store.query(query, n_results=top_k)

            if not results:
                return ToolResult(
                    status=ToolResultStatus.SUCCESS,
                    output=f"æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„å†…å®¹",
                    data={"results": [], "query": query},
                )

            output_lines = [f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç‰‡æ®µï¼š\n"]
            data_results = []

            for i, result in enumerate(results, 1):
                # è·å–æ–‡ä»¶å
                filename = result.metadata.get("filename", "æœªçŸ¥")
                chunk_idx = result.metadata.get("chunk_index", 0)

                output_lines.append(f"--- ç›¸å…³ç‰‡æ®µ {i} ---")
                output_lines.append(f"æ¥æº: {filename}")
                output_lines.append(f"å†…å®¹: {result.text[:300]}...")
                output_lines.append("")

                data_results.append({
                    "filename": filename,
                    "chunk_index": chunk_idx,
                    "text": result.text,
                    "distance": result.distance,
                })

            return ToolResult(
                status=ToolResultStatus.SUCCESS,
                output="\n".join(output_lines),
                data={"results": data_results, "query": query},
            )

        except Exception as e:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"æœç´¢å¤±è´¥: {e}",
            )

    def _query_document(self, params: dict[str, Any]) -> ToolResult:
        """æŸ¥è¯¢æŒ‡å®šæ–‡æ¡£ã€‚"""
        doc_name = params.get("document_name", "").strip()
        query = params.get("query", "").strip()
        top_k = params.get("top_k", 3)

        if not doc_name or not query:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error="æ–‡æ¡£åå’ŒæŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º",
            )

        # æŸ¥æ‰¾æ–‡æ¡£
        import sqlite3

        conn = sqlite3.connect(self._db_path)
        try:
            row = conn.execute(
                "SELECT id, filename FROM rag_documents WHERE filename LIKE ? LIMIT 1",
                (f"%{doc_name}%",),
            ).fetchone()
        finally:
            conn.close()

        if not row:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"æœªæ‰¾åˆ°æ–‡æ¡£: {doc_name}",
            )

        doc_id, filename = row

        # æŸ¥è¯¢å‘é‡åº“
        try:
            results = self.vector_store.query_by_document(query, doc_id, top_k)

            if not results:
                return ToolResult(
                    status=ToolResultStatus.SUCCESS,
                    output=f"åœ¨ {filename} ä¸­æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„å†…å®¹",
                    data={"filename": filename},
                )

            output_lines = [f"åœ¨ {filename} ä¸­æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç‰‡æ®µï¼š\n"]
            data_results = []

            for i, result in enumerate(results, 1):
                output_lines.append(f"--- ç‰‡æ®µ {i} ---")
                output_lines.append(f"å†…å®¹: {result.text[:300]}...")
                output_lines.append("")

                data_results.append({
                    "text": result.text,
                    "distance": result.distance,
                })

            return ToolResult(
                status=ToolResultStatus.SUCCESS,
                output="\n".join(output_lines),
                data={"filename": filename, "results": data_results},
            )

        except Exception as e:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error=f"æŸ¥è¯¢å¤±è´¥: {e}",
            )

    def _list_documents(self, params: dict[str, Any]) -> ToolResult:
        """åˆ—å‡ºæ–‡æ¡£ã€‚"""
        limit = min(params.get("limit", 50), 200)

        import sqlite3

        conn = sqlite3.connect(self._db_path)
        try:
            rows = conn.execute(
                """SELECT id, filename, original_path, stored_path, file_type, file_size, content_text, chunk_count, indexed_at
                   FROM rag_documents ORDER BY indexed_at DESC LIMIT ?""",
                (limit,),
            ).fetchall()
        finally:
            conn.close()

        if not rows:
            return ToolResult(
                status=ToolResultStatus.SUCCESS,
                output="çŸ¥è¯†åº“ä¸­æš‚æ— æ–‡æ¡£ï¼Œè¯·ä½¿ç”¨ add_document æ·»åŠ æ–‡æ¡£",
                data={"documents": [], "count": 0},
            )

        lines = [f"çŸ¥è¯†åº“ä¸­å…± {len(rows)} ä¸ªæ–‡æ¡£ï¼š\n"]
        docs = []

        for i, (doc_id, filename, original_path, stored_path, file_type, size, content_text, chunks, indexed) in enumerate(rows, 1):
            size_kb = size / 1024
            # chunk_count=0 è¡¨ç¤ºè§£æå¤±è´¥
            if chunks == 0:
                lines.append(
                    f"  {i}. {filename} ({file_type}, {size_kb:.1f}KB) - âš ï¸ è§£æå¤±è´¥ï¼Œå†…å®¹ä¸ºç©º"
                )
            else:
                lines.append(
                    f"  {i}. {filename} ({file_type}, {size_kb:.1f}KB, {chunks}å—)"
                )
            docs.append({
                "id": doc_id,
                "filename": filename,
                "original_path": original_path,
                "stored_path": stored_path,
                "file_type": file_type,
                "size": size,
                "content_text": content_text or "",
                "chunk_count": chunks,
                "indexed_at": indexed,
            })

        return ToolResult(
            status=ToolResultStatus.SUCCESS,
            output="\n".join(lines),
            data={"documents": docs, "count": len(docs)},
        )

    def _remove_document(self, params: dict[str, Any]) -> ToolResult:
        """åˆ é™¤æ–‡æ¡£ã€‚"""
        doc_id = params.get("document_id")

        if doc_id is None:
            return ToolResult(
                status=ToolResultStatus.ERROR,
                error="ç¼ºå°‘ document_id",
            )

        import sqlite3

        conn = sqlite3.connect(self._db_path)
        try:
            # è·å–æ–‡æ¡£ä¿¡æ¯
            row = conn.execute(
                "SELECT filename, stored_path FROM rag_documents WHERE id = ?",
                (doc_id,),
            ).fetchone()

            if not row:
                return ToolResult(
                    status=ToolResultStatus.ERROR,
                    error=f"æ–‡æ¡£ä¸å­˜åœ¨: ID {doc_id}",
                )

            filename, stored_path = row

            # åˆ é™¤å‘é‡åº“ä¸­çš„å—
            self.vector_store.delete_by_document(doc_id)

            # åˆ é™¤å­˜å‚¨çš„æ–‡ä»¶
            try:
                Path(stored_path).unlink(missing_ok=True)
            except Exception:
                pass

            # åˆ é™¤æ•°æ®åº“è®°å½•
            conn.execute("DELETE FROM rag_documents WHERE id = ?", (doc_id,))
            conn.commit()

        finally:
            conn.close()

        return ToolResult(
            status=ToolResultStatus.SUCCESS,
            output=f"å·²ä»çŸ¥è¯†åº“åˆ é™¤: {filename}",
            data={"document_id": doc_id, "deleted": True},
        )

